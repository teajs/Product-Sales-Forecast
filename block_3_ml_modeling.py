# -*- coding: utf-8 -*-
"""Block 3 ML Modeling.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZojeAH01lhvSQqZPhGpLDP2Ufh3m5-ce
"""

#import Libraries
import pandas as pd
import pandas as np

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_squared_log_error
from statsmodels.tsa.statespace.sarimax import SARIMAX
from sklearn.ensemble import VotingRegressor
import matplotlib.pyplot as plt
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import cross_val_score
from xgboost import XGBRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error

# Load the datasets


test_data = pd.read_csv('TEST_FINAL.csv')
train_data = pd.read_csv('TRAIN.csv')

# Display the first few rows of each dataset to understand their structure
test_data_head = test_data.head()
train_data_head = train_data.head()

test_data_info = test_data.info()
train_data_info = train_data.info()

test_data_head, train_data_head, test_data_info, train_data_info

"""1.1 Data Processing.

   Data Cleaning
"""

# Check for missing values
train_missing_values = train_data.isnull().sum()
test_missing_values = test_data.isnull().sum()

# Remove duplicates
train_data = train_data.drop_duplicates()
test_data = test_data.drop_duplicates()

train_missing_values, test_missing_values

"""1.2.Feature Engineering"""

# Convert 'Date' to datetime
train_data['Date'] = pd.to_datetime(train_data['Date'])
test_data['Date'] = pd.to_datetime(test_data['Date'])

# Create new time-based features
train_data['Day'] = train_data['Date'].dt.day
train_data['Month'] = train_data['Date'].dt.month
train_data['Year'] = train_data['Date'].dt.year
train_data['DayOfWeek'] = train_data['Date'].dt.dayofweek
train_data['Quarter'] = train_data['Date'].dt.quarter

test_data['Day'] = test_data['Date'].dt.day
test_data['Month'] = test_data['Date'].dt.month
test_data['Year'] = test_data['Date'].dt.year
test_data['DayOfWeek'] = test_data['Date'].dt.dayofweek
test_data['Quarter'] = test_data['Date'].dt.quarter

# Example of creating a lag feature (previous day's sales)
train_data['Lag_Sales_1'] = train_data.groupby('Store_id')['Sales'].shift(1)

# Drop rows with NaN values generated from lag features
train_data = train_data.dropna()

train_data.head()

# Check the column names in the training dataset
train_data.columns

# Check the column names in the training dataset
train_data.columns.tolist()

# Verify that 'Sales' column is present
if 'Sales' not in train_data.columns:
    raise KeyError("The 'Sales' column is not present in the training dataset.")

# Convert 'Date' to datetime
train_data['Date'] = pd.to_datetime(train_data['Date'])
test_data['Date'] = pd.to_datetime(test_data['Date'])

# Create new time-based features
train_data['Day'] = train_data['Date'].dt.day
train_data['Month'] = train_data['Date'].dt.month
train_data['Year'] = train_data['Date'].dt.year
train_data['DayOfWeek'] = train_data['Date'].dt.dayofweek
train_data['Quarter'] = train_data['Date'].dt.quarter

# Example of creating a lag feature (previous day's sales)
train_data['Lag_Sales_1'] = train_data.groupby('Store_id')['Sales'].shift(1)

# Drop rows with NaN values generated from lag features
train_data = train_data.dropna()

train_data.head()

"""1.3.Data Transformation

# New section
"""

# Define categorical and numerical columns
categorical_cols = ['Store_Type', 'Location_Type', 'Region_Code', 'Discount']
numerical_cols = ['#Order', 'Day', 'Month', 'Year', 'DayOfWeek', 'Quarter', 'Lag_Sales_1']

# Preprocessing pipeline
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numerical_cols),
        ('cat', OneHotEncoder(), categorical_cols)
    ])

# Prepare data for model
X_train = train_data.drop(['ID', 'Date', 'Sales'], axis=1)
y_train = train_data['Sales']

# Verify column names in X_train
X_train.columns.tolist()

# Fit and transform the data
X_train_preprocessed = preprocessor.fit_transform(X_train)

# Display the shape of the preprocessed data
X_train_preprocessed.shape

"""1.4Train-Test Split and Model Training


"""

#from sklearn.model_selection import train_test_split

# Split the training data
X_train_split, X_val, y_train_split, y_val = train_test_split(X_train_preprocessed, y_train, test_size=0.2, random_state=42)

# Baseline Model: Linear Regression
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error

# Train the model
linear_regressor = LinearRegression()
linear_regressor.fit(X_train_split, y_train_split)

# Make predictions
y_pred_train = linear_regressor.predict(X_train_split)
y_pred_val = linear_regressor.predict(X_val)

# Evaluate the model
mae_train = mean_absolute_error(y_train_split, y_pred_train)
mse_train = mean_squared_error(y_train_split, y_pred_train)
rmse_train = mean_squared_error(y_train_split, y_pred_train, squared=False)

mae_val = mean_absolute_error(y_val, y_pred_val)
mse_val = mean_squared_error(y_val, y_pred_val)
rmse_val = mean_squared_error(y_val, y_pred_val, squared=False)

mae_train, mse_train, rmse_train, mae_val, mse_val, rmse_val

"""2. Model Selection



#   Baseline Model: Linear Regression
"""

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error

linear_regressor = LinearRegression()
linear_regressor.fit(X_train_split, y_train_split)

y_pred_train = linear_regressor.predict(X_train_split)
y_pred_val = linear_regressor.predict(X_val)

mae_train = mean_absolute_error(y_train_split, y_pred_train)
mse_train = mean_squared_error(y_train_split, y_pred_train)
rmse_train = mean_squared_error(y_train_split, y_pred_train, squared=False)

mae_val = mean_absolute_error(y_val, y_pred_val)
mse_val = mean_squared_error(y_val, y_pred_val)
rmse_val = mean_squared_error(y_val, y_pred_val, squared=False)

print(f'Baseline Model Train MAE: {mae_train}, MSE: {mse_train}, RMSE: {rmse_train}')
print(f'Baseline Model Validation MAE: {mae_val}, MSE: {mse_val}, RMSE: {rmse_val}')

# Train the model
linear_regressor = LinearRegression()
linear_regressor.fit(X_train_split, y_train_split)

# Make predictions
y_pred_train = linear_regressor.predict(X_train_split)
y_pred_val = linear_regressor.predict(X_val)

# Evaluate the model
mae_train = mean_absolute_error(y_train_split, y_pred_train)
mse_train = mean_squared_error(y_train_split, y_pred_train)
rmse_train = mean_squared_error(y_train_split, y_pred_train, squared=False)

mae_val = mean_absolute_error(y_val, y_pred_val)
mse_val = mean_squared_error(y_val, y_pred_val)
rmse_val = mean_squared_error(y_val, y_pred_val, squared=False)

mae_train, mse_train, rmse_train, mae_val, mse_val, rmse_val

"""Exploring Other Models ( XGBoost)"""

# Initialize and train the XGBoost model
xgb_model = XGBRegressor()
xgb_model.fit(X_train_split, y_train_split)

# Make predictions
y_pred_train_xgb = xgb_model.predict(X_train_split)
y_pred_val_xgb = xgb_model.predict(X_val)

# Evaluate the XGBoost model
mae_train_xgb = mean_absolute_error(y_train_split, y_pred_train_xgb)
mse_train_xgb = mean_squared_error(y_train_split, y_pred_train_xgb)
rmse_train_xgb = mean_squared_error(y_train_split, y_pred_train_xgb, squared=False)

mae_val_xgb = mean_absolute_error(y_val, y_pred_val_xgb)
mse_val_xgb = mean_squared_error(y_val, y_pred_val_xgb)
rmse_val_xgb = mean_squared_error(y_val, y_pred_val_xgb, squared=False)

print(f'XGBoost Train MAE: {mae_train_xgb}')
print(f'XGBoost Train MSE: {mse_train_xgb}')
print(f'XGBoost Train RMSE: {rmse_train_xgb}')
print(f'XGBoost Validation MAE: {mae_val_xgb}')
print(f'XGBoost Validation MSE: {mse_val_xgb}')
print(f'XGBoost Validation RMSE: {rmse_val_xgb}')

""". Hyperparameter Tuning using Grid Search"""

# Define parameter grid for XGBoost
param_grid = {
    'n_estimators': [100, 200],
    'learning_rate': [0.01, 0.1],
    'max_depth': [3, 5]
}

# Initialize GridSearchCV
grid_search = GridSearchCV(estimator=XGBRegressor(), param_grid=param_grid, cv=3, scoring='neg_mean_squared_error', n_jobs=-1)

# Fit GridSearchCV
grid_search.fit(X_train_split, y_train_split)

# Best parameters and score
print(f'Best parameters: {grid_search.best_params_}')
print(f'Best score: {grid_search.best_score_}')

"""Cross-Validation"""

# Perform cross-validation
cv_scores = cross_val_score(XGBRegressor(), X_train_preprocessed, y_train, cv=5, scoring='neg_mean_squared_error')

# Convert to positive scores
cv_scores = -cv_scores
print(f'Cross-validation scores: {cv_scores}')
print(f'Average CV score: {cv_scores.mean()}')

""" Feature Importance with Tree-Based Models"""

# Fit the model
xgb_model = XGBRegressor()
xgb_model.fit(X_train_split, y_train_split)

# Get feature importances
importances = xgb_model.feature_importances_

# Plot feature importances
features = preprocessor.transformers_[1][1].get_feature_names_out(categorical_cols)  # OneHotEncoder feature names
all_features = numerical_cols + list(features)
plt.figure(figsize=(10, 6))
plt.barh(all_features, importances)
plt.xlabel('Feature Importance')
plt.title('Feature Importances from XGBoost')
plt.show()

"""Ensemble Methods"""

# Initialize base models
linear_regressor = LinearRegression()
xgb_model = XGBRegressor()

# Create Voting Regressor
voting_regressor = VotingRegressor(estimators=[
    ('linear', linear_regressor),
    ('xgb', xgb_model)
])

# Train Voting Regressor
voting_regressor.fit(X_train_split, y_train_split)

# Make predictions
y_pred_train_voting = voting_regressor.predict(X_train_split)
y_pred_val_voting = voting_regressor.predict(X_val)

# Evaluate the Voting Regressor
mae_train_voting = mean_absolute_error(y_train_split, y_pred_train_voting)
mse_train_voting = mean_squared_error(y_train_split, y_pred_train_voting)
rmse_train_voting = mean_squared_error(y_train_split, y_pred_train_voting, squared=False)

mae_val_voting = mean_absolute_error(y_val, y_pred_val_voting)
mse_val_voting = mean_squared_error(y_val, y_pred_val_voting)
rmse_val_voting = mean_squared_error(y_val, y_pred_val_voting, squared=False)

print(f'Voting Regressor Train MAE: {mae_train_voting}')
print(f'Voting Regressor Train MSE: {mse_train_voting}')
print(f'Voting Regressor Train RMSE: {rmse_train_voting}')
print(f'Voting Regressor Validation MAE: {mae_val_voting}')
print(f'Voting Regressor Validation MSE: {mse_val_voting}')
print(f'Voting Regressor Validation RMSE: {rmse_val_voting}')

""" Seasonality Handling with SARIMA"""

# Prepare data for SARIMA
train_data_sarima = train_data.set_index('Date')
train_data_sarima = train_data_sarima[['Sales']]

# Initialize SARIMA model
sarima_model = SARIMAX(train_data_sarima, order=(1, 1, 1), seasonal_order=(1, 1, 1, 12))

# Fit SARIMA model
sarima_result = sarima_model.fit()

# Make predictions
y_pred_sarima = sarima_result.predict(start=train_data_sarima.index[0], end=train_data_sarima.index[-1])

# Evaluate SARIMA
mae_sarima = mean_absolute_error(train_data_sarima['Sales'], y_pred_sarima)
mse_sarima = mean_squared_error(train_data_sarima['Sales'], y_pred_sarima)
rmse_sarima = mean_squared_error(train_data_sarima['Sales'], y_pred_sarima, squared=False)

print(f'SARIMA MAE: {mae_sarima}')
print(f'SARIMA MSE: {mse_sarima}')
print(f'SARIMA RMSE: {rmse_sarima}')